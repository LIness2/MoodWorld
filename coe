import spacy

# Charger le modèle spaCy
nlp = spacy.load("en_core_web_sm")

# Exemple de texte
text = "I am so happy today! I hope you are doing well."

# Traiter le texte
doc = nlp(text)

# Afficher les tokens, les lemmes, et les parties du discours
for token in doc:
    print(f"Texte : {token.text}, Lemme : {token.lemma_}, POS : {token.pos_}")



# Nettoyage du texte (enlever stopwords et ponctuation)
cleaned_text = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
print("Texte nettoyé :", " ".join(cleaned_text))


import pandas as pd

# Exemple de plusieurs textes
texts = ["I am so happy today!", "This is a very frustrating situation."]
cleaned_texts = []

for text in texts:
    doc = nlp(text)
    cleaned_text = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    cleaned_texts.append(" ".join(cleaned_text))



# Créer un DataFrame
df = pd.DataFrame({'text': texts, 'cleaned_text': cleaned_texts})
print(df)


#préparer les donnés : Exemple de jeu de données
# Exemple de données
texts = [
    "I am so happy today!",  # Joie
    "This is so frustrating!",  # Colère
    "I am scared about the future.",  # Peur
    "I feel hopeful for tomorrow.",  # Espoir
    "This is so sad and disappointing."  # Tristesse
    "I am neutral about this situation."  # Neutre
]

# Labels des émotions (0: Joie, 1: Colère, 2: Peur, 3: Espoir, 4: Tristesse, 5: Neutre)
labels = [0, 1, 2, 3, 4, 5]
from sklearn.model_selection import train_test_split

# Diviser en données d'entraînement et de validation
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)

from transformers import BertTokenizer

# Charger le tokenizer de BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokeniser les textes d'entraînement et de validation
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

from datasets import Dataset

# Créer les datasets pour l'entraînement et la validation
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels
})

val_dataset = Dataset.from_dict({
    'input_ids': val_encodings['input_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': val_labels
})

from transformers import BertForSequenceClassification

# Charger le modèle pré-entraîné BERT pour la classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)
# num_labels=5 est utilisé car nous avons 5 classes d'émotions.


from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch

# Définir les arguments d'entraînement
training_args = TrainingArguments(
    output_dir='./results',          # Où les résultats seront sauvegardés
    num_train_epochs=3,              # Nombre d'époques d'entraînement
    per_device_train_batch_size=8,   # Taille du batch pour l'entraînement
    per_device_eval_batch_size=16,   # Taille du batch pour l'évaluation
    warmup_steps=500,                # Nombre de pas avant de commencer à réduire le taux d'apprentissage
    weight_decay=0.01,               # Pénalité de poids
    logging_dir='./logs',            # Répertoire pour les logs
)

from transformers import Trainer

# Créer le Trainer avec les arguments d'entraînement
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Entraîner le modèle
trainer.train()

# Sauvegarder le modèle et le tokenizer
model.save_pretrained('./emotion_classifier')
tokenizer.save_pretrained('./emotion_classifier')


from transformers import BertForSequenceClassification, BertTokenizer

# Charger le modèle fine-tuné et le tokenizer
model = BertForSequenceClassification.from_pretrained('./emotion_classifier')
tokenizer = BertTokenizer.from_pretrained('./emotion_classifier')

# Exemple de texte à prédire
new_text = "I feel so hopeful about the future."

# Tokeniser le texte
inputs = tokenizer(new_text, return_tensors="pt", padding=True, truncation=True)

# Faire une prédiction
with torch.no_grad():
    logits = model(**inputs).logits

# Convertir les logits en classe d'émotion
predicted_class = torch.argmax(logits, dim=-1).item()

# Afficher la classe prédite
print(f"Emotion prédite : {predicted_class}")  # Affichera un numéro de classe (ex : 0 = Joie)

emotion_dict = {
    0: "Joie",
    1: "Colère",
    2: "Peur",
    3: "Espoir",
    4: "Tristesse"
    5: "Neutre" # type: ignore
}

# Afficher l'émotion prédite
print(f"Emotion prédite : {emotion_dict[predicted_class]}")

import matplotlib.pyplot as plt

# Dictionnaire des couleurs
emotion_colors = {
    0: "yellow",     # Joie
    1: "red",      # Colère
    2: "purple",   # Peur
    3: "blanc",    # Espoir
    4: "blue"    # Tristesse
    5: "grey"    # Neutre
}

# Obtenir la couleur associée à l'émotion prédite
emotion_color = emotion_colors[predicted_class]

# Afficher la couleur et l'émotion
print(f"Emotion : {emotion_dict[predicted_class]}")
print(f"Couleur : {emotion_color}")

# Afficher la couleur dans un graphique simple
plt.figure(figsize=(2, 2))
plt.imshow([[emotion_color]])
plt.axis('off')  # Cacher les axes
plt.title(f"Emotion: {emotion_dict[predicted_class]}")
plt.show()




# même code mais corrigé par chatgpt
import spacy

# Charger le modèle spaCy
nlp = spacy.load("en_core_web_sm")

# Exemple de texte
text = "I am so happy today! I hope you are doing well."

# Traiter le texte
doc = nlp(text)

# Afficher les tokens, les lemmes, et les parties du discours
for token in doc:
    print(f"Texte : {token.text}, Lemme : {token.lemma_}, POS : {token.pos_}")

# Nettoyage du texte (enlever stopwords et ponctuation)
cleaned_text = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
print("Texte nettoyé :", " ".join(cleaned_text))

import pandas as pd

# Exemple de plusieurs textes
texts = ["I am so happy today!", "This is a very frustrating situation."]
cleaned_texts = []

for text in texts:
    doc = nlp(text)
    cleaned_text = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    cleaned_texts.append(" ".join(cleaned_text))

# Créer un DataFrame
df = pd.DataFrame({'text': texts, 'cleaned_text': cleaned_texts})
print(df)

# Préparer les données : Exemple de jeu de données
texts = [
    "I am so happy today!",  # Joie
    "This is so frustrating!",  # Colère
    "I am scared about the future.",  # Peur
    "I feel hopeful for tomorrow.",  # Espoir
    "This is so sad and disappointing.",  # Tristesse
]

# Labels des émotions (0: Joie, 1: Colère, 2: Peur, 3: Espoir, 4: Tristesse)
labels = [0, 1, 2, 3, 4]
from sklearn.model_selection import train_test_split

# Diviser en données d'entraînement et de validation
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)

from transformers import BertTokenizer

# Charger le tokenizer de BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokeniser les textes d'entraînement et de validation
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

from datasets import Dataset

# Créer les datasets pour l'entraînement et la validation
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels
})

val_dataset = Dataset.from_dict({
    'input_ids': val_encodings['input_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': val_labels
})

from transformers import BertForSequenceClassification

# Charger le modèle pré-entraîné BERT pour la classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)

from transformers import TrainingArguments, Trainer
import torch

# Définir les arguments d'entraînement
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Créer le Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Entraîner le modèle
trainer.train()

# Sauvegarder le modèle et le tokenizer
model.save_pretrained('./emotion_classifier')
tokenizer.save_pretrained('./emotion_classifier')

# Charger le modèle fine-tuné et le tokenizer
model = BertForSequenceClassification.from_pretrained('./emotion_classifier')
tokenizer = BertTokenizer.from_pretrained('./emotion_classifier')

# Exemple de texte à prédire
new_text = "I feel so hopeful about the future."
inputs = tokenizer(new_text, return_tensors="pt", padding=True, truncation=True)

# Faire une prédiction
with torch.no_grad():
    logits = model(**inputs).logits

# Convertir les logits en classe d'émotion
predicted_class = torch.argmax(logits, dim=-1).item()

emotion_dict = {
    0: "Joie",
    1: "Colère",
    2: "Peur",
    3: "Espoir",
    4: "Tristesse",
}

# Afficher l'émotion prédite
print(f"Emotion prédite : {emotion_dict[predicted_class]}")

import matplotlib.pyplot as plt

# Dictionnaire des couleurs
emotion_colors = {
    0: "yellow",
    1: "red",
    2: "purple",
    3: "white",
    4: "blue",
}

# Obtenir la couleur associée à l'émotion prédite
emotion_color = emotion_colors[predicted_class]

# Afficher la couleur dans un graphique
plt.figure(figsize=(2, 2))
plt.imshow([[plt.cm.get_cmap('Pastel1')(predicted_class)]])
plt.axis('off')
plt.title(f"Emotion: {emotion_dict[predicted_class]}")
plt.show()
