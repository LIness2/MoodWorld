import requests
import subprocess
import pandas as pd
import matplotlib.pyplot as plt
from pymongo import MongoClient
from collections import Counter
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline
)
from datetime import datetime, timezone
import geocoder
from deep_translator import GoogleTranslator
import time
import warnings

from get_articles_scrapy import get_articles_scrapy

warnings.filterwarnings("ignore")

# === CONFIGURATION ===
GNEWS_API_KEY = "e275d891ea6264bcdc91bfd73567c65b"
GNEWS_API_URL = "https://gnews.io/api/v4/search"
USE_CUSTOM_MODEL = False

# === PIPELINES ===
emotion_pipeline = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    return_all_scores=True
)

try:
    custom_model = pipeline(
        "text-classification",
        model="./emotion_model",
        tokenizer="./emotion_model"
    )
except Exception:
    custom_model = None

# === OUTILS NLP ===
tokenizer_cleaner = AutoTokenizer.from_pretrained("camembert-base")

def clean_text(text: str) -> str:
    tokens = tokenizer_cleaner.tokenize(text)
    return " ".join(tokens)

# === MONGODB ===
client = MongoClient("mongodb://localhost:27017/")
db = client["articles_db"]
collection = db["articles"]

# === TRADUCTION ===
def translate_to_english(text: str, source_lang: str) -> str:
    if not text or source_lang == "en":
        return text
    try:
        return GoogleTranslator(source=source_lang, target='en').translate(text)
    except Exception:
        return text

# === API GNEWS ===
def get_articles_with_rate_limit(query: str = "politique", lang: str = "fr", max_articles: int = 10, delay: int = 5) -> list:
    fetched_articles = []
    while len(fetched_articles) < max_articles:
        params = {
            "q": query,
            "lang": lang,
            "max": max_articles,
            "token": GNEWS_API_KEY
        }
        response = requests.get(GNEWS_API_URL, params=params)
        if response.status_code != 200:
            print("Erreur GNews:", response.json())
            break

        fetched_articles.extend([
            {
                "title": a["title"],
                "author": a.get("source", {}).get("name", "Inconnu"),
                "publication_date": a["publishedAt"],
                "content": a.get("content", ""),
                "url": a["url"],
                "source": a.get("source", {}).get("name", "Inconnu"),
                "tags": query,
                "lang": lang
            }
            for a in response.json().get("articles", [])
        ])

        if len(fetched_articles) < max_articles:
            print(f"Attente de {delay} secondes pour respecter la limite des requ√™tes...")
            time.sleep(delay)

    return fetched_articles[:max_articles]

# === SAUVEGARDE ===
def save_articles_to_mongo(articles: list) -> None:
    for article in articles:
        if not collection.find_one({"title": article["title"]}):
            collection.insert_one(article)

# === ANALYSE & ANNOTATION ===
def annotate_articles_with_emotions(lang="fr"):
    articles = collection.find({"lang": lang})

    for article in articles:
        text = article.get("content", "")
        if not text:
            continue

        translated_text = translate_to_english(text, lang)

        pipeline_used = custom_model if USE_CUSTOM_MODEL and custom_model else emotion_pipeline
        emotion_results = pipeline_used(translated_text[:512])[0]
        emotion_results.sort(key=lambda x: x["score"], reverse=True)

        emotion_label = emotion_results[0]["label"]
        emotion_score = emotion_results[0]["score"]

        couleur = {
            "joy": "#fcd34d",
            "anger": "#f87171",
            "sadness": "#60a5fa",
            "fear": "#a78bfa",
            "surprise": "#34d399",
            "neutral": "#9ca3af"
        }.get(emotion_label.lower(), "#9ca3af")

        pays = geocoder.ip("me").country or "Inconnu"

        collection.update_one(
            {"_id": article["_id"]},
            {"$set": {
                "emotion": emotion_label,
                "score_emotion": emotion_score,
                "translated_text": translated_text,
                "couleur": couleur,
                "pays": pays,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }}
        )

        print(f"‚úÖ {emotion_label} ({lang} ‚Üí en | {pays}) - {article['title']}")

# === ENTRA√éNEMENT MOD√àLE ===
def train_custom_model(texts, labels, model_name="distilbert-base-uncased"):
    df = pd.DataFrame({"text": texts, "label": labels})
    df["cleaned_text"] = df["text"].apply(clean_text)

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    if df["label"].nunique() > 1 and df["label"].value_counts().min() > 1:
        stratify = df["label"]
    else:
        stratify = None

    train_df, val_df = train_test_split(df, test_size=0.2, stratify=stratify, random_state=42)

    train_dataset = Dataset.from_pandas(train_df)
    val_dataset = Dataset.from_pandas(val_df)

    train_dataset = train_dataset.map(lambda b: tokenizer(b["cleaned_text"], padding=True, truncation=True), batched=True)
    val_dataset = val_dataset.map(lambda b: tokenizer(b["cleaned_text"], padding=True, truncation=True), batched=True)

    return train_dataset, val_dataset

# === EX√âCUTION ===
if __name__ == "__main__":
    use_scrapy = True  # ‚Üê Choix de la m√©thode

    topics = ["politique", "√©cologie", "technologie"]
    langs = ["fr", "en"]

    for topic in topics:
        for lang in langs:
            print(f"\nüåç Collecte '{topic}' en {lang} via {'Scrapy' if use_scrapy else 'API'}")
            if use_scrapy:
                articles = get_articles_scrapy(max_articles=10)
            else:
                articles = get_articles_with_rate_limit(query=topic, lang=lang, max_articles=10)
            save_articles_to_mongo(articles)

    annotate_articles_with_emotions(lang="fr")

