import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from pymongo import MongoClient
from collections import Counter
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline)
from datetime import datetime
import geocoder
from deep_translator import GoogleTranslator
import warnings
warnings.filterwarnings("ignore")
import re
import nltk
nltk.download('stopwords')  # 👈 à exécuter une fois
from nltk.corpus import stopwords #temporairement
stop_words = set(stopwords.words('french'))
import os
print("Répertoire courant :", os.getcwd())
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json

# === CONFIGURATION ===
NEWSAPI_URL = "https://newsapi.org/api/v4/search"
NEWSAPI_KEY = "e275d891ea6264bcdc91bfd73567c65b"
USE_CUSTOM_MODEL = False  # Toggle modèle personnalisé

# === CHARGEMENT DES PIPELINES ===
emotion_pipeline =  pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)

try:
    custom_model = pipeline("text-classification", model="./emotion_model", tokenizer="./emotion_model")
except:
    custom_model = None

# === OUTILS NLP pour le nettoyage ===
tokenizer_cleaner = AutoTokenizer.from_pretrained("camembert-base")

stop_words = set(stopwords.words('french'))  # adapter à la langue

def clean_text(text):
    # passage en minuscules
    text = text.lower()
    # suppression ponctuation et caractères non alphabétiques
    text = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ\s]', '', text)
    # tokenization simple par espaces
    tokens = text.split()
    # suppression stopwords
    tokens = [t for t in tokens if t not in stop_words]
    # tu peux aussi utiliser tokenizer_cleaner.tokenize si tu veux
    return " ".join(tokens)

# === MONGODB ===
client = MongoClient("mongodb://localhost:27017/")
db = client["articles_db"]
collection = db["articles"]

# === TRADUCTION ===
def translate_to_english(text, source_lang):
    if not text or source_lang == "en":
        return text
    try:
        return GoogleTranslator(source=source_lang, target='en').translate(text)
    except:
        return text  # fallback: return as is


# Récupérer les articles depuis NewsAPI en paginant
def get_newsapi_articles(country, language, query="", max_pages=5):
    print(f"\n📰 NewsAPI - Articles pour {country} ({language})")

    total_articles = 0  # <- Il faut initialiser cette variable avant la boucle
    params = {
        "q": query,
        "language": language,
        "pageSize": 10,
        "apiKey": NEWSAPI_KEY
    }
    for page in range(1, max_pages + 1):
        params["page"] = page
        response = requests.get(NEWSAPI_URL, params=params)

        if response.status_code == 200:
            data = response.json()
            articles = data.get("articles", [])
            if not articles:
                break

            for article in articles:
                print(f"📌 {article['title']} ({article['source']['name']})")
                print(f"🔗 {article['url']}\n")
                total_articles += 1
        else:
            print(f"❌ Erreur {response.status_code}: {response.text}")
            break

    if total_articles == 0:
        print("❌ Aucun article trouvé.")


def save_articles_to_mongo(articles):
    count = 0
    if not articles:
        print("⚠️ Aucun article à sauvegarder.")
        return
    for a in articles:
        if not collection.find_one({"url": a["url"]}):
            collection.insert_one(a)
            count += 1
    print(f"{count} article(s) ajouté(s) dans MongoDB.")

# === EMOTIONS ===
def detect_emotion_custom(text):
    if custom_model is None:
        raise ValueError("Le modèle personnalisé n'est pas chargé.")
    return max(custom_model(text[:512]), key=lambda x: x['score'])["label"]

def detect_emotion(text):
    result = emotion_pipeline(text[:512])[0]
    return max(result, key=lambda x: x["score"])["label"]

def detect(text):
    if USE_CUSTOM_MODEL:
        if custom_model is not None:
            label = detect_emotion_custom(text)
            score = None
        else:
            print("⚠️ Custom model non chargé, utilisation du modèle standard")
            # fallback sur emotion_pipeline
            result = emotion_pipeline(text[:512])[0]
            sorted_res = sorted(result, key=lambda x: x["score"], reverse=True)
            label = sorted_res[0]["label"]
            score = round(sorted_res[0]["score"], 3)
    else:
        result = emotion_pipeline(text[:512])[0]
        sorted_res = sorted(result, key=lambda x: x["score"], reverse=True)
        label = sorted_res[0]["label"]
        score = round(sorted_res[0]["score"], 3)
    return label, score

def annotate_articles_with_emotions():
    for article in collection.find({"emotion": {"$exists": False}}):
        original_text = article.get("content") or article.get("description") or ""
        if not original_text:
            continue

        lang = article.get("lang", "en")
        translated_text = translate_to_english(original_text, source_lang=lang)

        if not translated_text.strip():
            continue

        emotion_label, emotion_score = detect(translated_text)

        couleur_map = {
            "joy": "jaune", "sadness": "bleu", "anger": "rouge",
            "fear": "violet", "love": "rose", "surprise": "orange"
        }
        couleur = couleur_map.get(emotion_label, "gris")
        pays = geocoder.ip('me').country or "Inconnu"

        collection.update_one(
            {"_id": article["_id"]},
            {"$set": {
                "emotion": emotion_label,
                "score_emotion": emotion_score,
                "translated_text": translated_text,
                "couleur": couleur,
                "pays": pays,
                "timestamp": datetime.utcnow().isoformat()
            }}
        )
        print(f"✅ {emotion_label} ({lang} ➜ en | {pays}) - {article['title']}")

# === MODEL TRAINING ===
def train_custom_model(texts, labels, model_name="distilbert-base-uncased"):
    df = pd.DataFrame({"text": texts, "label": labels})
    df["cleaned_text"] = df["text"].apply(clean_text)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
    train_dataset = Dataset.from_pandas(train_df)
    val_dataset = Dataset.from_pandas(val_df)
    train_dataset = train_dataset.map(lambda b: tokenizer(b["cleaned_text"], padding=True, truncation=True), batched=True)
    val_dataset = val_dataset.map(lambda b: tokenizer(b["cleaned_text"], padding=True, truncation=True), batched=True)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(labels)))
    args = TrainingArguments(output_dir="./emotion_model", num_train_epochs=3, per_device_train_batch_size=4)
    trainer = Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=tokenizer)
    trainer.train()
    model.save_pretrained("./emotion_model")
    tokenizer.save_pretrained("./emotion_model")

# === VISUALISATION ===
def plot_emotions():
    emotions = [a["emotion"] for a in collection.find({"emotion": {"$exists": True}})]
    counts = Counter(emotions)
    plt.bar(counts.keys(), counts.values(), color="orange")
    plt.title("Distribution des émotions")
    plt.xlabel("Émotion")
    plt.ylabel("Articles")
    plt.show()

def show_articles_by_emotion(emotion):
    print(f"\nArticles avec l'émotion : {emotion}")
    for a in collection.find({"emotion": emotion}):
        print(f"• {a['title']} ({a['source']})")

# === CONFIGURATION MULTI-THEMES / MULTI-LANGUES ===
# config.py
THEMES = ["politique", "Ukraine", "Guerre", "culture", "économie", "Israël"]

LANGUAGES = ["fr", "en", "de", "he", "ze", "en", "es", "it", "pt", "ru", "zh", "ar", "fa"]

TOPICS_BY_COUNTRY = {
    "France": ("fr", "politique OR économie OR Ukraine OR Israël"),
    "USA": ("en", "election OR Trump OR economy OR Gaza"),
    "Germany": ("de", "wirtschaft OR politik OR ukraine OR israel"),
    "Israel": ("he", "נתניהו OR עזה OR פוליטיקה OR ביטחון"),
    "China": ("zh", "政治 OR 经济 OR 台湾 OR 美国"),
    "Russia": ("ru", "политика OR экономика OR Украина OR США"),
    "Nigeria": ("en", "election OR Buhari OR economy OR conflict"),
    "South Africa": ("en", "Ramaphosa OR politics OR economy OR BRICS"),
    "Egypt": ("ar", "السيسي OR سياسة OR اقتصاد OR غزة"),
    "Brazil": ("pt", "política OR economia OR Lula OR Bolsonaro"),
    "India": ("en", "Modi OR BJP OR Kashmir OR economy"),
    "Pakistan": ("en", "Imran Khan OR politics OR military OR Kashmir"),
    "Iran": ("fa", "رئیسی OR سیاست OR اقتصاد OR اسرائیل"),
    "Saudi Arabia": ("ar", "بن سلمان OR سياسة OR اقتصاد OR نفط")
}
# === SCRIPT PRINCIPAL ===

def run_country_specific_scraping():
    for country, (lang, query) in TOPICS_BY_COUNTRY.items():
        print(f"\n🌍 Récupération des articles pour {country} [{lang}] avec les mots-clés : {query}")
        articles = get_newsapi_articles(country=country, language=lang, query=query, max_pages=5)
        save_articles_to_mongo(articles)

def run_general_theme_scraping():
    for theme in THEMES:
        for lang in LANGUAGES:
            print(f"\n📰 Recherche générale : thème '{theme}' en [{lang}]")
            articles = get_newsapi_articles(country="", language=lang, query=theme, max_pages=1)
            save_articles_to_mongo(articles)

#Création d’un DataFrame depuis MongoDB
def get_articles_dataframe():
    cursor = collection.find({"emotion": {"$exists": True}})
    articles = list(cursor)
    df = pd.DataFrame(articles)

    # Nettoyage et formatage des colonnes utiles
    df = df[["pays", "emotion", "score_emotion", "tags", "timestamp"]].copy()
    df["date"] = pd.to_datetime(df["timestamp"]).dt.date.astype(str)
    df["nb_articles"] = 1
    df.rename(columns={"pays": "country", "tags": "theme"}, inplace=True)
    return df

def export_cleaned_articles():
    cursor = collection.find({"emotion": {"$exists": True}})
    articles = list(cursor)
    with open("cleaned_articles.json", "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

st.button("Lancer le scraping")
run_country_specific_scraping()
run_general_theme_scraping()

# === Interface Streamlit
st.set_page_config(layout="wide")
st.title("🌍 WorldMood - Analyse émotionnelle de la presse mondiale")

# === Chargement des données ===
@st.cache_data
def load_data():
    with open("cleaned_articles.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    return pd.DataFrame(data)

@st.cache_resource
def load_emotion_model():
    return pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=1)

# === Analyse émotionnelle locale (si besoin) ===
def analyze_emotions(df, emotion_analyzer):
    if "timestamp" not in df.columns:
        df["timestamp"] = datetime.utcnow().isoformat()
    
    if "theme" not in df.columns:
        df["theme"] = "inconnu"

    df = df.copy()
    if "emotion" not in df.columns:
        df["emotion"] = df["content"].apply(lambda text: emotion_analyzer(text[:512])[0]["label"] if text else "unknown")
    df["year"] = pd.to_datetime(df["timestamp"]).dt.year
    df["date"] = pd.to_datetime(df["timestamp"]).dt.date.astype(str)
    df["nb_articles"] = 1
    return df

# === Visualisations ===
def bubble_map(df, selected_theme, year):
    filtered = df[(df["theme"] == selected_theme) & (df["year"] == year)]
    emotion_counts = filtered.groupby(["country", "emotion"]).size().reset_index(name="count")
    emotion_summary = emotion_counts.groupby("country")["count"].sum().reset_index()

    fig = px.scatter_geo(
        emotion_summary,
        locations="country",
        locationmode="country names",
        size="count",
        projection="natural earth",
        title=f"Distribution des émotions pour le thème '{selected_theme}' en {year}"
    )
    return fig

def contour_plot(df, countries, selected_theme, year_range):
    filtered = df[(df["theme"] == selected_theme) &
                  (df["country"].isin(countries)) &
                  (df["year"].between(*year_range))]
    
    grouped = filtered.groupby(["year", "country", "emotion"]).size().reset_index(name="count")
    pivot = grouped.pivot_table(index="year", columns=["country", "emotion"], values="count", fill_value=0)
    
    z = pivot.values
    x = pivot.index.values
    y = [f"{col[0]}-{col[1]}" for col in pivot.columns]

    fig = go.Figure(data=go.Contour(z=z, x=x, y=y, colorscale="Viridis", contours_coloring='heatmap'))
    fig.update_layout(title="Évolution des émotions par pays et par année",
                      xaxis_title="Année", yaxis_title="Pays-Émotion")
    return fig

# === MAIN ===
raw_data = load_data()
emotion_pipeline = load_emotion_model()
data = analyze_emotions(raw_data, emotion_pipeline)

# === Sidebar ===
theme = st.sidebar.selectbox("🎯 Choisissez un thème", sorted(data["theme"].unique()))
years = sorted(data["year"].dropna().unique())
year = st.sidebar.selectbox("🗓️ Année pour la carte à bulles", years)
year_range = st.sidebar.slider("⏳ Plage d'années pour comparaison", int(min(years)), int(max(years)), (int(min(years)), int(max(years))))

# === Graph 1 ===
st.subheader("📍 Carte mondiale des émotions")
st.plotly_chart(bubble_map(data, theme, year), use_container_width=True)

# === Graph 2 ===
st.subheader("📈 Comparaison temporelle par pays")
countries = st.multiselect("Choisissez jusqu’à 3 pays", sorted(data["country"].unique()), max_selections=3)
if countries:
    st.plotly_chart(contour_plot(data, countries, theme, year_range), use_container_width=True)

if st.sidebar.button("💾 Exporter en JSON"):
    export_cleaned_articles()
    st.success("✅ Export vers cleaned_articles.json terminé.")



