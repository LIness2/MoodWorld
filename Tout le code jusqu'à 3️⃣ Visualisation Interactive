import requests 

# Cl√© API pour NewsAPI
NEWSAPI_KEY = "4aa659261bd2443997ffc38ef5c851fd"
NEWSAPI_URL = "https://newsapi.org/v2/everything"

# URL de GDELT (version 2.0)
GDELT_URL = "https://api.gdeltproject.org/api/v2/doc/doc"

# Liste des pays et langues
country_language_map = {
    "fr": "fr",
    "us": "en",
    "gb": "en",
    "de": "de",
    "es": "es",
    "it": "it",
    "ar": "ar",
}

# Param√®tres NewsAPI
newsapi_params = {
    "apiKey": NEWSAPI_KEY,
    "q": "Ukraine OR Russia OR Israel OR Palestine OR Inflation OR Economy",
    "pageSize": 5,
    "sortBy": "publishedAt",
    "from": "2025-03-20",
    "to": "2025-03-25",
}

# Fonction pour r√©cup√©rer les articles depuis NewsAPI
def get_newsapi_articles(country, language):
    params = newsapi_params.copy()
    params["language"] = language
    response = requests.get(NEWSAPI_URL, params=params)

    print(f"\nüì∞ NewsAPI - Articles pour {country} ({language})")
    if response.status_code == 200:
        data = response.json()
        if data.get("articles"):
            for article in data["articles"]:
                print(f"üìå {article['title']} ({article['source']['name']})")
                print(f"üîó {article['url']}\n")
        else:
            print("‚ùå Aucun article trouv√©.")
    else:
        print(f"‚ùå Erreur {response.status_code}: {response.text}")

# Fonction pour r√©cup√©rer les articles depuis GDELT
def get_gdelt_articles():
    params = {
        "query": "Ukraine OR Russia OR Israel OR Palestine OR Inflation OR Economy",
        "mode": "ArtList",
        "maxrecords": 10,  # Augment√© pour plus d'articles
        "format": "json",
        "sort": "date",  # Trier par date
    }
    response = requests.get(GDELT_URL, params=params)

    print("\nüåç GDELT - Articles d'actualit√© mondiale")
    if response.status_code == 200:
        data = response.json()
        if "articles" in data:
            for article in data["articles"]:
                print(f"üìå {article.get('title', 'Sans titre')} ({article.get('source', 'Inconnu')})")
                print(f"üîó {article.get('url', 'Pas de lien')}\n")
        else:
            print("‚ùå Aucun article trouv√©.")
    else:
        print(f"‚ùå Erreur {response.status_code}: {response.text}")

# R√©cup√©rer les articles pour chaque pays (NewsAPI)
for country, language in country_language_map.items():
    get_newsapi_articles(country, language)

# R√©cup√©rer les articles depuis GDELT
get_gdelt_articles()

import requests
from pymongo import MongoClient

API_KEY = "4aa659261bd2443997ffc38ef5c851fd"  # Remplace par ta cl√© API
URL = "https://newsapi.org/v2/everything"

def get_articles(query="politique", language="fr", page_size=10):
    """ R√©cup√®re des articles d‚Äôactualit√© et les structure en JSON. """
    params = {
        "q": query,
        "language": language,
        "pageSize": page_size,
        "apiKey": API_KEY
    }

    response = requests.get(URL, params=params)
    data = response.json()

    if data["status"] != "ok":
        print("Erreur :", data)
        return []

    # Structuration des articles sous forme de documents JSON
    articles = [
        {
            "title": article["title"],
            "author": article.get("author", "Inconnu"),
            "publication_date": article["publishedAt"],
            "content": article.get("content", "Aucun contenu"),
            "url": article["url"],
            "source": article["source"]["name"],
            "tags": query
        }
        for article in data["articles"]
    ]

    return articles  # Retourne une liste de dictionnaires JSON

# Test : r√©cup√©rer 5 articles
articles_json = get_articles("climat", "fr", 5)
print(articles_json)

def save_articles_to_mongo(articles):
    """ Sauvegarde une liste d'articles dans MongoDB sans doublons (via l'URL). """
    client = MongoClient('mongodb://localhost:27017/')
    db = client['articles_db']
    articles_collection = db['articles']

    count = 0
    for article in articles:
        if not articles_collection.find_one({"url": article["url"]}):
            articles_collection.insert_one(article)
            count += 1

    if count:
        print(f"{count} article(s) sauvegard√©(s) dans MongoDB.")
    else:
        print("‚úÖ Aucun nouvel article √† enregistrer (tout est d√©j√† en base).")


# √âtape 1 ‚Äî Nettoyer le texte avec spaCy
import spacy

nlp = spacy.load("fr_core_news_sm")  # si tu analyses des articles FR

def clean_text(text):
    doc = nlp(text)
    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]
    return " ".join(tokens)

# Pr√©paration du dataset supervis√©

from sklearn.model_selection import train_test_split
import pandas as pd

# Ton dataframe de base
data = {
    "text": [
        "I love this product!",
        "This is awful and I hate it.",
        "It‚Äôs okay, nothing special.",
        "Amazing quality, would buy again.",
        "Worst experience ever."
    ],
    "label": ["joy", "anger", "neutral", "joy", "anger"]
}
df = pd.DataFrame(data)
df["cleaned_text"] = df["text"].apply(clean_text)

# Split en 80% train / 20% test
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)


# Tokenisation avec DistilBERT

from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Split le dataframe
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# Conversion en datasets
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# Tokenization
def tokenize(batch):
    return tokenizer(batch["cleaned_text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)



# Entra√Æner le mod√®le DistilBERT pour la classification des √©motions

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=3  # selon ton dataset
)

training_args = TrainingArguments(
    output_dir="./emotion_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs"
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)


trainer.train()

# Sauvegarder le mod√®le: Int√©gration MongoDB avec ton mod√®le

from transformers import pipeline

emotion_model = pipeline("text-classification", model="./emotion_model", tokenizer=tokenizer)

def detect_emotion_custom(text):
    scores = emotion_model(text[:512])
    return max(scores, key=lambda x: x["score"])["label"]


#√âtape 2 ‚Äî Analyser les √©motions des articles avec le mod√®le distilroberta
# √Ä int√©grer juste apr√®s la sauvegarde MongoDB.

from transformers import pipeline

# Initialisation du mod√®le d'analyse d'√©motion

emotion_classifier = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    return_all_scores=True
)

def detect_emotion(text):
    """Retourne l‚Äô√©motion dominante d‚Äôun texte"""
    if not text:
        return "Neutral"
    scores = emotion_classifier(text[:512])  # Limite pour √©viter d√©passement token
    top_emotion = max(scores[0], key=lambda x: x["score"])
    return top_emotion["label"]

def annotate_articles_with_emotions():
    """Ajoute une √©motion dominante aux articles"""
    client = MongoClient('mongodb://localhost:27017/')
    db = client['articles_db']
    collection = db['articles']

    for article in collection.find({"emotion": {"$exists": False}}):
        text = article.get("content", "") or article.get("description", "")
        emotion = detect_emotion(text)
        collection.update_one(
            {"_id": article["_id"]},
            {"$set": {"emotion": emotion}}
        )
        print(f"üé≠ √âmotion d√©tect√©e : {emotion} ‚Üí Article : {article['title']}")

annotate_articles_with_emotions()

# √âtape 3 ‚Äî Afficher les √©motions avec matplotlib

import matplotlib.pyplot as plt
from collections import Counter

def plot_emotions():
    """Affiche la distribution des √©motions"""
    client = MongoClient('mongodb://localhost:27017/')
    db = client['articles_db']
    collection = db['articles']

    emotions = [doc['emotion'] for doc in collection.find({"emotion": {"$exists": True}})]
    count = Counter(emotions)

    plt.figure(figsize=(8, 5))
    plt.bar(count.keys(), count.values(), color='coral')
    plt.title("R√©partition des √©motions dans les articles")
    plt.xlabel("√âmotions")
    plt.ylabel("Occurrences")
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

plot_emotions()


# afficher un top des titres pour chaque √©motion d√©tect√©e

def show_articles_by_emotion(emotion):
    articles = collection.find({"emotion": emotion})
    print(f"\nüì∞ Articles avec l‚Äô√©motion : {emotion}")
    for article in articles:
        print(f"‚Ä¢ {article['title']} ({article['source']})")


# R√©sum√© des √©tapes √† suivre dans l'ordre :

articles_json = get_articles("climat", "fr", 5)
save_articles_to_mongo(articles_json)

annotate_articles_with_emotions()
plot_emotions()




